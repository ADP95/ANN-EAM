{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba1f4b8-f01e-4380-b691-9d49addb4aed",
   "metadata": {
    "id": "04ab4fbd-f191-427a-a2e4-db4ff76fa385"
   },
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5816c3bf",
   "metadata": {
    "executionInfo": {
     "elapsed": 3102,
     "status": "ok",
     "timestamp": 1651313252929,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "42e504d1-66bf-459a-bdee-ef0f268a7b81"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cmdstanpy \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import os\n",
    "import json\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4c972-c98d-4af8-bc2c-77a913eb654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"] + plt.rcParams[\"font.serif\"]\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['pdf.use14corefonts'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5ed11-783c-476d-b190-95c7356783ca",
   "metadata": {},
   "source": [
    "## Choose Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14c0fa-9b17-4f5c-b596-34067f92acc0",
   "metadata": {},
   "source": [
    "#### roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb77e1-704f-4cd9-b93d-cbb56ee67c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './'\n",
    "plots_root = root + 'Plots/'\n",
    "datasets_root = root + 'Datasets/'\n",
    "behavioural_data_root = root +  'behavioral_data/selected_data/' \n",
    "stan_files_root = root +  'stan files/' \n",
    "saved_models_root = root + 'stan_results/'\n",
    "\n",
    "model_config = {}\n",
    "plots_path = ''\n",
    "dataset_path = ''\n",
    "stan_file_path = ''\n",
    "stan_output_dir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c82f7e-d49b-4814-b33c-6b556f20d7ba",
   "metadata": {},
   "source": [
    "#### read models configuration json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56040ecb-0f2e-4816-81f7-dd0b3086b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lba_models.json\") as f:\n",
    "    models = json.load(f)\n",
    "    models_name = list(models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96889d-a548-4827-9666-b093ff1e6f4c",
   "metadata": {},
   "source": [
    "#### Choose and set model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9681a08e-5ec2-4cef-b88e-b7ad4ebcfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetModelAndPaths(model_name):\n",
    "    global model_config\n",
    "    global plots_path\n",
    "    global dataset_path\n",
    "    global stan_file_path\n",
    "    global stan_output_dir\n",
    "    model_config = models[model_name]\n",
    "    plots_path = plots_root + model_config['plots_folder_name'] + '/'\n",
    "    dataset_path = datasets_root + model_config['dataset_name']\n",
    "    stan_file_path = stan_files_root + model_config['stan_file']\n",
    "    stan_output_dir = saved_models_root + model_config['model_name'] + '/'\n",
    "    os.path\n",
    "    \n",
    "    if not os.path.exists(plots_path):\n",
    "        os.makedirs(plots_path)\n",
    "        print(\"Directory \" , plots_path ,  \" Created \")\n",
    "    else:    \n",
    "        print(\"Directory \" , plots_path ,  \" already exists\")\n",
    "        \n",
    "    if not os.path.exists(stan_output_dir):\n",
    "        os.makedirs(stan_output_dir)\n",
    "        print(\"Directory \" , stan_output_dir ,  \" Created \")\n",
    "    else:    \n",
    "        print(\"Directory \" , stan_output_dir ,  \" already exists\")\n",
    "\n",
    "widgets.interact(SetModelAndPaths, model_name=models_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01260a8a-f9e1-458f-be62-3b63bd53cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e617f6",
   "metadata": {
    "id": "123c4809-7b46-4f8d-b578-0d5e9fb5fbe7"
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d41247-a08b-4710-a8cb-006ca5ace314",
   "metadata": {},
   "source": [
    "Loading words and non-words with zipf and predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6417ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1651313252934,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "72172233-0e82-4058-8a5c-8657e9fe4693",
    "outputId": "35336463-3bb2-41e5-c84f-7c34fdcfc77d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_nword_df = pd.read_csv(dataset_path, header=None, names =['string', 'freq',  'label', 'zipf','category', 'word_prob', 'non_word_prob'])\n",
    "word_nword_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a061e32-1d3d-40c2-afcf-04de82e44ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, max_rt, min_rt, std_c=2.5):\n",
    "    \"\"\"\n",
    "    Returns remove outliers from dataframes. Outlier RTs are bigger than\n",
    "    max_rt and smaller than min_rt. Also RTsthat are out of -/+ (std_c * sd) \n",
    "    of mean RT interval are considered as outliers too.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df: pandas dataframe with rt column\n",
    "        max_rt (float): maximum acceptable rt\n",
    "        min_rt (float): minimum acceptable rt\n",
    "        \n",
    "    Optional Parameters\n",
    "    ----------\n",
    "        std_c (float) : Optional\n",
    "            coefficient to define interval of non-outlier RTs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        df: pandas dataframe without outliers  \n",
    "    \"\"\"\n",
    "    mean = df['rt'].mean()\n",
    "    sd = df['rt'].std()\n",
    "    lower_thr = mean - std_c*sd\n",
    "    upper_thr = mean + std_c*sd\n",
    "    min_bound = max(min_rt, lower_thr)\n",
    "    max_bound = min(max_rt, upper_thr)\n",
    "    df = df[df['rt'] >= min_bound]\n",
    "    df = df[df['rt'] <= max_bound]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf7c47-7fd4-4bdb-880b-5a52a2b7be4b",
   "metadata": {},
   "source": [
    "Reading and modifing each behavioral data file and combining all of them into a single behavioral dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a914e",
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1651313252936,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "eb864830-ecb7-48f1-a071-f00173154216"
   },
   "outputs": [],
   "source": [
    "Number_Of_Participants = 5\n",
    "Number_Of_Trials = 400\n",
    "dataframes = []\n",
    "\n",
    "for i in range(Number_Of_Participants):\n",
    "    # Loading each file\n",
    "    df = pd.read_csv(behavioural_data_root + str(i+1) + \"DATA.LDT\", names=['trial', 'string_id', 'string_type', 'accuracy', 'rt', 'string'])\n",
    "    # Dropping non rows and first two rows that are demographic informations \n",
    "    df = df.dropna().drop('string_id', axis=1).drop([0, 1]).iloc[:Number_Of_Trials] \n",
    "    # Converting columns type to suitable data types\n",
    "    convert_dict = {'string_type': 'int16',\n",
    "                    'accuracy': 'int16',\n",
    "                    'rt': float\n",
    "                   }\n",
    "\n",
    "    df = df.astype(convert_dict)\n",
    "    # Convert RTs to seconds\n",
    "    df['rt'] = df['rt'].apply(lambda x: x/1000) \n",
    "    # Removing Outliers\n",
    "    df = remove_outliers(df, 3, .2, 2.5)\n",
    "    # Extracting response of participant from his/her accuracy\n",
    "    df['response'] = np.logical_not(np.logical_xor(df['string_type'], df['accuracy'])).astype('int')\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Particpant number\n",
    "    df['participant'] = i+1\n",
    "    # Minimum RT of participant in all trials (is needed for stan code)\n",
    "    df['minRT'] = df['rt'].min()\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6266a",
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1651313252938,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "f37d4118-f2ea-4691-bff9-02f1ac1cebbc"
   },
   "outputs": [],
   "source": [
    "# Combining dataframes\n",
    "behavioural_df = pd.concat(dataframes)\n",
    "# Merging  behavioral dataframe with word_nonword_df to have words and non-words data with behavioral data\n",
    "behavioural_df = pd.merge(behavioural_df, word_nword_df, on='string', how='left').dropna().reset_index(drop=True)\n",
    "behavioural_df = behavioural_df.drop([\"trial\", \"string_type\", \"freq\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c791ac-26e5-4ee4-929e-379e1b3248df",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioural_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159dddd5-c8a8-4c1a-8c48-9882ee452eb2",
   "metadata": {},
   "source": [
    "Predicted probabilities of words and non-words in different conditions in all trials\n",
    "across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1441c1-4928-4675-92a0-bac19c71fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioural_df.groupby(['category']).agg({'word_prob': ['mean', 'std', 'count', 'max', 'min'], 'non_word_prob': ['mean', 'std', 'count', 'max', 'min']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052f5c8-5b10-4c7d-ad5c-27e781c292fb",
   "metadata": {},
   "source": [
    "RT and response description of words and non-words in different conditions in all trials\n",
    "across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3741e-8006-4460-a23d-67445b949a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioural_df.groupby(['category']).agg({'rt': ['mean', 'std', 'max', 'min'], 'response': ['mean', 'std', 'max', 'min']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6ff4c",
   "metadata": {
    "id": "ff61f25e-7817-4f3f-a48f-d5a6666b0e75",
    "tags": []
   },
   "source": [
    "## Stan Model and Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa0a87-ed95-486a-beec-87a50c2f86ef",
   "metadata": {},
   "source": [
    "Compiling stan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60550ee7-157d-4393-9f12-773eb947efce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93380,
     "status": "ok",
     "timestamp": 1651313346844,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "c1b3627d-0ea2-49b6-86f0-7b098a5a16d6",
    "outputId": "4bc8643c-82c0-4fc9-99a0-7f232fa1eea4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lba_model = cmdstanpy.CmdStanModel(model_name=model_config['model_name'],\n",
    "                                   stan_file=stan_file_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c8503-9cb2-40c0-ae79-e0e8d386a86f",
   "metadata": {},
   "source": [
    "Preparing model's inputs\n",
    "\n",
    "note that some inputs of data_dict might not be used depending on which model is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361fc9e-7a35-41fc-a377-3b7547842c61",
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1651313346849,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "3e9f42b0-a7b3-476d-8869-958b2155b84c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = len(behavioural_df)                                                    # For all models\n",
    "participant = behavioural_df['participant'].to_numpy()                     # For all models\n",
    "p = behavioural_df.loc[:, ['word_prob', 'non_word_prob']].to_numpy()       # predicted probabilites of words and non-words, for ANN-EAM models\n",
    "frequency = behavioural_df['zipf'].to_numpy().astype(int)                  # zipf values For models with non-decision time or drift modulation\n",
    "frequencyCondition = behavioural_df['category'].replace([\"HF\", \"LF\", \"NW\"], [1, 2, 3]).to_numpy() # For models with conditional drift\n",
    "response = behavioural_df['response'].to_numpy().astype(int)               # for all models\n",
    "rt = behavioural_df['rt'].to_numpy()                                       # for all models\n",
    "minRT = behavioural_df['minRT'].to_numpy()                                 # for all models\n",
    "RTbound = 0.1                                                              # for all models\n",
    "\n",
    "k_priors = [0, 1, 1, 1]                  # All models with LBA\n",
    "A_priors = [1, 2, 1, 1]                  # All models wtih LBA\n",
    "ndt_priors = [0, 1, 1, 1];               # For models wtihout non-decision time modulation\n",
    "g_priors = [-2, 1, 0, 1]                 # For models wtih non-decision time modulation\n",
    "m_priors = [0, 0.5, 0, 1]                # For models wtih non-decision time modulation\n",
    "drift_priors = [1, 2, 1, 1]              # For models withoud drift mapping functions (non ANN-EAM models)\n",
    "alpha_priors = [0, 1, 1, 1]              # For models with drift mapping functions\n",
    "b_priors = [0, 1, 1, 1]                  # For models with drift mapping functions with asymptote modulation and linear models\n",
    "# There is a k parameter in LBA impelemention so we use theta as k parameter in sigmoid function\n",
    "theta_priors = [2, 1, 1, 1]              # For models with sigmoid drift mapping functions (ANN-EAM models) (equivalent of k_priors in RDM and ANN-RDM)\n",
    "\n",
    "# define input for the model\n",
    "data_dict = {'N': N,\n",
    "             'L': Number_Of_Participants,\n",
    "             'participant': participant,\n",
    "             'response': response,\n",
    "             'rt': rt,\n",
    "             'minRT': minRT,\n",
    "             'RTbound': RTbound,\n",
    "             'frequency': frequency,\n",
    "             'frequencyCondition': frequencyCondition,\n",
    "             'k_priors': k_priors,\n",
    "             'A_priors': A_priors,\n",
    "             'ndt_priors': ndt_priors,\n",
    "             'g_priors': g_priors,\n",
    "             'm_priors': m_priors,\n",
    "             'drift_priors': drift_priors,\n",
    "             'p': p,\n",
    "             'alpha_priors': alpha_priors,\n",
    "             'b_priors': b_priors,\n",
    "             'k_priors': k_priors,\n",
    "             'theta_priors': theta_priors,\n",
    "             }\n",
    "\n",
    "# set sampling parameters\n",
    "n_iter = 500\n",
    "n_warmup = int(n_iter/2)\n",
    "n_sample = int(n_iter/2)\n",
    "n_chains = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4034d3d-99d8-43bf-b744-6b0608141c8d",
   "metadata": {},
   "source": [
    "Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddd68e",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "edac015c-18f4-4ebf-8c3c-ecaf4b8e1d6a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit = lba_model.sample(data=data_dict,\n",
    "                       iter_sampling=n_sample, \n",
    "                       iter_warmup=n_warmup,\n",
    "                       chains=n_chains,\n",
    "                       output_dir=stan_output_dir,\n",
    "                       show_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7bfe3-f4b0-4c5e-b38a-da896fc9910a",
   "metadata": {},
   "source": [
    "## Model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4b9f0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a8e1d56e-e7e4-4ca1-a0bd-5b34edf6ea44"
   },
   "outputs": [],
   "source": [
    "print(\"***hmc diagnostics:\")\n",
    "print(fit.diagnose(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033c45b",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "efa2a6ff-ad48-4c7e-a094-4c645923644e"
   },
   "outputs": [],
   "source": [
    "df = fit.summary()\n",
    "\n",
    "print(\"***DF: \")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302aa3b",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "b33a48e1-8c3b-4668-9e74-ba74c58e3bec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "print(\"***Rhat > 1.01: \")\n",
    "for f in df[\"R_hat\"]:\n",
    "    if f >= 1.01 or f <= 0.9:\n",
    "        counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54606c2d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "681ed2bf-2a28-4a8b-90d0-24b9fede846f"
   },
   "outputs": [],
   "source": [
    "df.loc[df['R_hat']>1.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa820a",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a651e5b7-b7f1-4426-ae8e-f9e236eb6e91"
   },
   "outputs": [],
   "source": [
    "df.loc[df['R_hat']>1.01].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32ef5c",
   "metadata": {
    "id": "2743cc7e-7238-4f84-afe4-4b502770f62b",
    "tags": []
   },
   "source": [
    "## Check parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aaf784-e97d-4d81-bcb9-90e7eefff98f",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33d647",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "d781cdef-a1c6-4af1-940c-824398ec7d2b"
   },
   "outputs": [],
   "source": [
    "# fit = cmdstanpy.from_csv(stan_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc740f54-db93-410c-9e26-282710ef2024",
   "metadata": {},
   "source": [
    "Parameters posterior plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67254033",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "48fc6630-74be-47a0-b387-ec99b9dc4b47"
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(fit, var_names=model_config['transf_params'],\n",
    "                  hdi_prob=.95);\n",
    "plt.savefig(plots_path + 'Parameters.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c0a9fe",
   "metadata": {},
   "source": [
    "Loading model parameters for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133df71d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3539c070-29f7-422d-8c2e-1f7d44ef9246"
   },
   "outputs": [],
   "source": [
    "drift_word_t = fit.stan_variables()['drift_word_t']\n",
    "drift_nonword_t = fit.stan_variables()['drift_nonword_t']\n",
    "ndt_t = fit.stan_variables()['ndt_t']\n",
    "if model_config['model_name'] != \"LBA\":\n",
    "    k_t_word = fit.stan_variables()['k_t_word']\n",
    "    k_t_nonword = fit.stan_variables()['k_t_nonword']\n",
    "    A_t_word = fit.stan_variables()['A_t_word']\n",
    "    A_t_nonword = fit.stan_variables()['A_t_nonword']\n",
    "\n",
    "else:\n",
    "    k_t = fit.stan_variables()['k_t']\n",
    "    A_t = fit.stan_variables()['A_t']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaeee5d-28e4-4b67-9d90-6d8ba5d39423",
   "metadata": {},
   "source": [
    "#### Models mean parameters in different conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645f86a",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3a247d68-c3af-49ef-91fd-ba4ea221358d"
   },
   "outputs": [],
   "source": [
    "v_HF_condition_w = drift_word_t[:, behavioural_df['category']==\"HF\"]\n",
    "v_HF_condition_nw = drift_nonword_t[:, behavioural_df['category']==\"HF\"]\n",
    "v_LF_condition_w = drift_word_t[:, behavioural_df['category']==\"LF\"]\n",
    "v_LF_condition_nw = drift_nonword_t[:, behavioural_df['category']==\"LF\"]\n",
    "v_NW_condition_w = drift_word_t[:, behavioural_df['category']==\"NW\"]\n",
    "v_NW_condition_nw = drift_nonword_t[:, behavioural_df['category']==\"NW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e000d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "deac1183-5610-4a19-8ff8-7bace8d12e7c"
   },
   "outputs": [],
   "source": [
    "print('HF words, word drift mean and std:')\n",
    "print(np.mean(np.mean(v_HF_condition_w, axis=1)), np.std(np.mean(v_HF_condition_w, axis=1)))\n",
    "print('HF words, nonword drift mean and std:')\n",
    "print(np.mean(np.mean(v_HF_condition_nw, axis=1)), np.std(np.mean(v_HF_condition_nw, axis=1)))\n",
    "print('LF words word drift mean and std:')\n",
    "print(np.mean(np.mean(v_LF_condition_w, axis=1)), np.std(np.mean(v_LF_condition_w, axis=1)))\n",
    "print('LF words nonword drift mean and std:')\n",
    "print(np.mean(np.mean(v_LF_condition_nw, axis=1)), np.std(np.mean(v_LF_condition_nw, axis=1)))\n",
    "print('NW words word drift mean and std:')\n",
    "print(np.mean(np.mean(v_NW_condition_w, axis=1)), np.std(np.mean(v_NW_condition_w, axis=1)))\n",
    "print('NW words nonword drift mean and std:')\n",
    "print(np.mean(np.mean(v_NW_condition_nw, axis=1)), np.std(np.mean(v_NW_condition_nw, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f925d27-4954-4ff6-b0b9-b299da5b855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"LBA\":\n",
    "    A_HF_condition_w = A_t_word[:, behavioural_df['category']==\"HF\"]\n",
    "    A_HF_condition_nw = A_t_nonword[:, behavioural_df['category']==\"HF\"]\n",
    "    A_LF_condition_w = A_t_word[:, behavioural_df['category']==\"LF\"]\n",
    "    A_LF_condition_nw = A_t_nonword[:, behavioural_df['category']==\"LF\"]\n",
    "    A_NW_condition_w = A_t_word[:, behavioural_df['category']==\"NW\"]\n",
    "    A_NW_condition_nw = A_t_nonword[:, behavioural_df['category']==\"NW\"]\n",
    "else:\n",
    "    A_HF_condition = A_t[:, behavioural_df['category']==\"HF\"]\n",
    "    A_LF_condition = A_t[:, behavioural_df['category']==\"LF\"]\n",
    "    A_NW_condition = A_t[:, behavioural_df['category']==\"NW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d462da-a2d7-403e-9e79-1bcfd84251db",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "deac1183-5610-4a19-8ff8-7bace8d12e7c"
   },
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"LBA\":\n",
    "    print('HF words, word starting point mean and std:')\n",
    "    print(np.mean(np.mean(A_HF_condition_w, axis=1)), np.std(np.mean(A_HF_condition_w, axis=1)))\n",
    "    print('HF words, nonword starting point mean and std:')\n",
    "    print(np.mean(np.mean(A_HF_condition_nw, axis=1)), np.std(np.mean(A_HF_condition_nw, axis=1)))\n",
    "    print('LF words word starting point mean and std:')\n",
    "    print(np.mean(np.mean(A_LF_condition_w, axis=1)), np.std(np.mean(A_LF_condition_w, axis=1)))\n",
    "    print('LF words nonword starting point mean and std:')\n",
    "    print(np.mean(np.mean(A_LF_condition_nw, axis=1)), np.std(np.mean(A_LF_condition_nw, axis=1)))\n",
    "    print('NW words word starting point mean and std:')\n",
    "    print(np.mean(np.mean(A_NW_condition_w, axis=1)), np.std(np.mean(A_NW_condition_w, axis=1)))\n",
    "    print('NW words nonword starting point mean and std:')\n",
    "    print(np.mean(np.mean(A_NW_condition_nw, axis=1)), np.std(np.mean(A_NW_condition_nw, axis=1)))\n",
    "else:\n",
    "    print('HF words, starting point mean and std:')\n",
    "    print(np.mean(np.mean(A_HF_condition, axis=1)), np.std(np.mean(A_HF_condition, axis=1)))\n",
    "    print('LF words starting point mean and std:')\n",
    "    print(np.mean(np.mean(A_LF_condition, axis=1)), np.std(np.mean(A_LF_condition, axis=1)))\n",
    "    print('NW words starting point mean and std:')\n",
    "    print(np.mean(np.mean(A_NW_condition, axis=1)), np.std(np.mean(A_NW_condition, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84308d46-e368-491f-8532-0f2b7b4f9966",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3a247d68-c3af-49ef-91fd-ba4ea221358d"
   },
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"LBA\":\n",
    "    k_HF_condition_w = k_t_word[:, behavioural_df['category']==\"HF\"]\n",
    "    k_HF_condition_nw = k_t_nonword[:, behavioural_df['category']==\"HF\"]\n",
    "    k_LF_condition_w = k_t_word[:, behavioural_df['category']==\"LF\"]\n",
    "    k_LF_condition_nw = k_t_nonword[:, behavioural_df['category']==\"LF\"]\n",
    "    k_NW_condition_w = k_t_word[:, behavioural_df['category']==\"NW\"]\n",
    "    k_NW_condition_nw = k_t_nonword[:, behavioural_df['category']==\"NW\"]\n",
    "\n",
    "    t_HF_condition_w = A_HF_condition_w + k_HF_condition_w\n",
    "    t_HF_condition_nw = A_HF_condition_nw + k_HF_condition_nw\n",
    "    t_LF_condition_w = A_LF_condition_w + k_LF_condition_w\n",
    "    t_LF_condition_nw = A_LF_condition_nw + k_LF_condition_nw\n",
    "    t_NW_condition_w = A_NW_condition_w + k_NW_condition_w\n",
    "    t_NW_condition_nw = A_NW_condition_nw + k_NW_condition_nw\n",
    "else:\n",
    "    k_HF_condition = k_t[:, behavioural_df['category']==\"HF\"]\n",
    "    k_LF_condition = k_t[:, behavioural_df['category']==\"LF\"]\n",
    "    k_NW_condition = k_t[:, behavioural_df['category']==\"NW\"]\n",
    "\n",
    "    t_HF_condition = A_HF_condition + k_HF_condition\n",
    "    t_LF_condition = A_LF_condition + k_LF_condition\n",
    "    t_NW_condition = A_NW_condition + k_NW_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc156d7-a7d2-4923-8a6f-c11ac57e3dd3",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "deac1183-5610-4a19-8ff8-7bace8d12e7c"
   },
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"LBA\":\n",
    "    print('HF words, word threshold mean and std:')\n",
    "    print(np.mean(np.mean(t_HF_condition_w, axis=1)), np.std(np.mean(t_HF_condition_w, axis=1)))\n",
    "    print('HF words, nonword threshold mean and std:')\n",
    "    print(np.mean(np.mean(t_HF_condition_nw, axis=1)), np.std(np.mean(t_HF_condition_nw, axis=1)))\n",
    "    print('LF words word threshold mean and std:')\n",
    "    print(np.mean(np.mean(t_LF_condition_w, axis=1)), np.std(np.mean(t_LF_condition_w, axis=1)))\n",
    "    print('LF words nonword threshold mean and std:')\n",
    "    print(np.mean(np.mean(t_LF_condition_nw, axis=1)), np.std(np.mean(t_LF_condition_nw, axis=1)))\n",
    "    print('NW words word threshold mean and std:')\n",
    "    print(np.mean(np.mean(t_NW_condition_w, axis=1)), np.std(np.mean(t_NW_condition_w, axis=1)))\n",
    "    print('NW words nonword threshold mean and std:')\n",
    "    print(np.mean(np.mean(t_NW_condition_nw, axis=1)), np.std(np.mean(t_NW_condition_nw, axis=1)))\n",
    "else:\n",
    "    print('HF words,  threshold mean and std:')\n",
    "    print(np.mean(np.mean(t_HF_condition, axis=1)), np.std(np.mean(t_HF_condition, axis=1)))\n",
    "    print('LF words threshold mean and std:')\n",
    "    print(np.mean(np.mean(t_LF_condition, axis=1)), np.std(np.mean(t_LF_condition, axis=1)))\n",
    "    print('NW words word threshold mean and std:')\n",
    "    print(np.mean(np.mean(t_NW_condition, axis=1)), np.std(np.mean(t_NW_condition, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f66b5-e6ae-4c65-b141-bd3f585724d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"LBA\":\n",
    "    bias_HF_condition_w = t_HF_condition_w - A_HF_condition_w\n",
    "    bias_HF_condition_nw = t_HF_condition_nw - A_HF_condition_nw\n",
    "    bias_LF_condition_w = t_LF_condition_w - A_LF_condition_w\n",
    "    bias_LF_condition_nw = t_LF_condition_nw - A_LF_condition_nw\n",
    "    bias_NW_condition_w = t_NW_condition_w - A_NW_condition_w\n",
    "    bias_NW_condition_nw = t_NW_condition_nw - A_NW_condition_nw\n",
    "else:\n",
    "    bias_HF_condition = t_HF_condition - A_HF_condition\n",
    "    bias_LF_condition = t_LF_condition - A_LF_condition\n",
    "    bias_NW_condition = t_NW_condition - A_NW_condition\n",
    "    bias_NW_condition = t_NW_condition - A_NW_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4871fb-0872-41ac-8677-1f37ec77a871",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "deac1183-5610-4a19-8ff8-7bace8d12e7c"
   },
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"LBA\":\n",
    "    print('HF words, word bias mean and std:')\n",
    "    print(np.mean(np.mean(bias_HF_condition_w, axis=1)), np.std(np.mean(bias_HF_condition_w, axis=1)))\n",
    "    print('HF words, nonword bias mean and std:')\n",
    "    print(np.mean(np.mean(bias_HF_condition_nw, axis=1)), np.std(np.mean(bias_HF_condition_nw, axis=1)))\n",
    "    print('LF words word bias mean and std:')\n",
    "    print(np.mean(np.mean(bias_LF_condition_w, axis=1)), np.std(np.mean(bias_LF_condition_w, axis=1)))\n",
    "    print('LF words nonword bias mean and std:')\n",
    "    print(np.mean(np.mean(bias_LF_condition_nw, axis=1)), np.std(np.mean(bias_LF_condition_nw, axis=1)))\n",
    "    print('NW words word bias mean and std:')\n",
    "    print(np.mean(np.mean(bias_NW_condition_w, axis=1)), np.std(np.mean(bias_NW_condition_w, axis=1)))\n",
    "    print('NW words nonword bias mean and std:')\n",
    "    print(np.mean(np.mean(bias_NW_condition_nw, axis=1)), np.std(np.mean(bias_NW_condition_nw, axis=1)))\n",
    "else:\n",
    "    print('HF words, bias mean and std:')\n",
    "    print(np.mean(np.mean(bias_HF_condition, axis=1)), np.std(np.mean(bias_HF_condition, axis=1)))\n",
    "    print('LF words bias mean and std:')\n",
    "    print(np.mean(np.mean(bias_LF_condition, axis=1)), np.std(np.mean(bias_LF_condition, axis=1)))\n",
    "    print('NW words bias mean and std:')\n",
    "    print(np.mean(np.mean(bias_NW_condition, axis=1)), np.std(np.mean(bias_NW_condition, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e74a9",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "06aceb06-2f53-47ae-b0f8-68d0c96f3cbf"
   },
   "outputs": [],
   "source": [
    "ndt_HF_condition = ndt_t[:, behavioural_df['category']==\"HF\"]\n",
    "ndt_LF_condition = ndt_t[:, behavioural_df['category']==\"LF\"]\n",
    "ndt_NW_condition = ndt_t[:, behavioural_df['category']==\"NW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacfbe0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "c7ff4f51-0587-4f7a-bf79-7bc1fe0951e2"
   },
   "outputs": [],
   "source": [
    "print('HF words ndt mean and std:')\n",
    "print(np.mean(np.mean(ndt_HF_condition, axis=1)), np.std(np.mean(ndt_HF_condition, axis=1)))\n",
    "print('LF words ndt mean and std:')\n",
    "print(np.mean(np.mean(ndt_LF_condition, axis=1)), np.std(np.mean(ndt_LF_condition, axis=1)))\n",
    "print('Non Words ndt mean and std:')\n",
    "print(np.mean(np.mean(ndt_NW_condition, axis=1)), np.std(np.mean(ndt_NW_condition, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64f181-de50-4bec-ad6a-9f0c7c95cea5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018e8df-553d-42b4-833c-d2f967b17ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_waic(log_likelihood, pointwise=False):\n",
    "    \"\"\"\n",
    "    Returns model comparisions' metrics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        log_likelihood: np.array\n",
    "            log_likelihood of each trial\n",
    "        max_rt: float\n",
    "            maximum acceptable rt\n",
    "        min_rt: float\n",
    "             minimum acceptable rt\n",
    "             \n",
    "    Optional Parameters\n",
    "    ----------------\n",
    "    pointwise: float\n",
    "        if true pointwise waic will be calculated\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        out:  a dictionary containing lppd, waic, waic_se and pointwise_waic    \n",
    "    \"\"\"\n",
    "    likelihood = np.exp(log_likelihood)\n",
    "\n",
    "    mean_l = np.mean(likelihood, axis=0) # N observations\n",
    "\n",
    "    pointwise_lppd = np.log(mean_l)\n",
    "    lppd = np.sum(pointwise_lppd)\n",
    "\n",
    "    pointwise_var_l = np.var(log_likelihood, axis=0) # N observations\n",
    "    var_l = np.sum(pointwise_var_l)\n",
    "\n",
    "    pointwise_waic = - 2*pointwise_lppd +  2*pointwise_var_l\n",
    "    waic = -2*lppd + 2*var_l\n",
    "    waic_se = np.sqrt(log_likelihood.shape[1] * np.var(pointwise_waic))\n",
    "\n",
    "    if pointwise:\n",
    "        out = {'lppd':lppd,\n",
    "               'p_waic':var_l,\n",
    "               'waic':waic,\n",
    "               'waic_se':waic_se,\n",
    "               'pointwise_waic':pointwise_waic}\n",
    "    else:\n",
    "        out = {'lppd':lppd,\n",
    "               'p_waic':var_l,\n",
    "                'waic':waic,\n",
    "                'waic_se':waic_se}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beabf75-4cb8-49dc-b604-9f0adf32dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = fit.stan_variables()['log_lik']\n",
    "print(calculate_waic(log_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23772b-94de-40e6-88d6-f5a6aaefab64",
   "metadata": {},
   "source": [
    "## Simulating LBA with estimated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737be2f5-fd9f-432a-a42f-c8bc89a28c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_lba_2A(k_word, A_word, k_nonword, A_nonword,\n",
    "                  tau, word_drift, nonword_drift):\n",
    "    shape = word_drift.shape\n",
    "    acc = np.empty(shape)\n",
    "    rt = np.empty(shape)\n",
    "    acc[:] = np.nan\n",
    "    rt[:] = np.nan\n",
    "\n",
    "    b_word = k_word + A_word\n",
    "    b_nonword = k_nonword + A_nonword\n",
    "    one_pose = True\n",
    "    v_word = np.random.normal(word_drift, np.ones(word_drift.shape))\n",
    "    v_nonword = np.random.normal(nonword_drift, np.ones(nonword_drift.shape))\n",
    "\n",
    "    while one_pose:\n",
    "        ind = np.logical_and(v_word < 0, v_nonword < 0)\n",
    "        v_word[ind] = np.random.normal(word_drift[ind], np.ones(word_drift[ind].shape))\n",
    "        v_nonword[ind] = np.random.normal(nonword_drift[ind], np.ones(nonword_drift[ind].shape))\n",
    "        one_pose = np.sum(ind)>0\n",
    "\n",
    "    start_word = np.random.uniform(np.zeros(A_word.shape), A_word)\n",
    "    start_nonword = np.random.uniform(np.zeros(A_nonword.shape), A_nonword)\n",
    "\n",
    "    ttf_word = (b_word - start_word) / v_word\n",
    "    ttf_nonword = (b_nonword - start_nonword) / v_nonword\n",
    "\n",
    "    ind = np.logical_and(ttf_word <= ttf_nonword, 0 < ttf_word)\n",
    "    acc[ind] = 1\n",
    "    rt[ind] = ttf_word[ind] + tau[ind]\n",
    "\n",
    "    ind = np.logical_and(ttf_nonword < 0, 0 < ttf_word)\n",
    "    acc[ind] = 1\n",
    "    rt[ind] = ttf_word[ind] + tau[ind]\n",
    "\n",
    "    ind = np.logical_and(ttf_nonword < ttf_word, 0 < ttf_nonword)\n",
    "    acc[ind] = 0\n",
    "    rt [ind] = ttf_nonword[ind] + tau[ind]\n",
    "\n",
    "    ind = np.logical_and(ttf_word < 0, 0 < ttf_nonword)\n",
    "    acc[ind] = 0\n",
    "    rt [ind] = ttf_nonword[ind] + tau[ind]\n",
    "\n",
    "    return rt, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867e05c-ebda-4bc3-ae51-e340e337344f",
   "metadata": {},
   "source": [
    "Simulating RDM with estimated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6786ed3-265f-4698-9a37-75d19e125806",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"LBA\":\n",
    "    pp_rt, pp_response = random_lba_2A(k_t_word, A_t_word, k_t_nonword, A_t_nonword, ndt_t, drift_word_t, drift_nonword_t)\n",
    "else:\n",
    "    pp_rt, pp_response = random_lba_2A(k_t, A_t, k_t, A_t, ndt_t, drift_word_t, drift_nonword_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7183cf1-fc0e-436b-b607-9d821ef736f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bci(x, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate Bayesian credible interval (BCI).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        An array containing MCMC samples.\n",
    "    \n",
    "    Optional Parameters\n",
    "    -------------------\n",
    "    alpha : float, default 0.05\n",
    "        Desired probability of type I error.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    interval : numpy.ndarray\n",
    "        Array containing the lower and upper bounds of the bci interval.\n",
    "    \"\"\"\n",
    "    interval = np.nanpercentile(x, [(alpha/2)*100, (1-alpha/2)*100])\n",
    "\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235c705-7301-4c3c-b906-5979789d606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Data\n",
    "rt_predictions = pd.concat((pd.DataFrame(pp_rt, index=pd.Index(np.arange(1, len(pp_rt)+1))).T, behavioural_df['category']), axis=1)\n",
    "response_predictions = pd.concat((pd.DataFrame(pp_response, index=pd.Index(np.arange(1, len(pp_response)+1))).T, behavioural_df['category']), axis=1)\n",
    "\n",
    "# Experiment Data\n",
    "experiment_data = behavioural_df.loc[:, ['rt', 'response', 'category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd192d-bac0-4fdc-afd2-148b6a9ae332",
   "metadata": {},
   "source": [
    "Separating RT and Response of predicted and experimental data for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd58c1-f474-4c7c-85d6-f26b16c77859",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_data = experiment_data.loc[experiment_data['category']=='HF']\n",
    "LF_data = experiment_data.loc[experiment_data['category']=='LF']\n",
    "NW_data = experiment_data.loc[experiment_data['category']=='NW']\n",
    "\n",
    "HF_pred_rt = rt_predictions.loc[rt_predictions['category']=='HF'].drop(['category'], axis=1)\n",
    "HF_pred_resp = response_predictions.loc[response_predictions['category']=='HF'].drop(['category'], axis=1)\n",
    "LF_pred_rt = rt_predictions.loc[rt_predictions['category']=='LF'].drop(['category'], axis=1)\n",
    "LF_pred_resp = response_predictions.loc[response_predictions['category']=='LF'].drop(['category'], axis=1)\n",
    "NW_pred_rt = rt_predictions.loc[rt_predictions['category']=='NW'].drop(['category'], axis=1)\n",
    "NW_pred_resp = response_predictions.loc[response_predictions['category']=='NW'].drop(['category'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68071a-3121-47da-94c5-bae5db334c5c",
   "metadata": {
    "id": "defea622-f638-4a0e-a269-937234d4a49f",
    "tags": []
   },
   "source": [
    "## Quantiles Posterior Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378eb36-76cc-4a39-b2ed-643addccbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [.1, .3, .5, .7, .9]\n",
    "percentiles = np.array(quantiles)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96001a51-55af-4f25-93c4-cdcc00556eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment Data quantile\n",
    "HF_quantile_ex = HF_data['rt'].quantile(quantiles)\n",
    "LF_quantile_ex = LF_data['rt'].quantile(quantiles)\n",
    "NW_quantile_ex = NW_data['rt'].quantile(quantiles)\n",
    "\n",
    "# predicted data quantiles (for each sample)\n",
    "HF_quantile_pred = HF_pred_rt.quantile(quantiles, axis=0).T\n",
    "LF_quantile_pred = LF_pred_rt.quantile(quantiles, axis=0).T\n",
    "NW_quantile_pred = NW_pred_rt.quantile(quantiles, axis=0).T\n",
    "\n",
    "# predicted data quantiles bci\n",
    "HF_predicted_bci = np.array([bci(HF_quantile_pred[x]) for x in quantiles])\n",
    "LF_predicted_bci = np.array([bci(LF_quantile_pred[x]) for x in quantiles])\n",
    "NW_predicted_bci = np.array([bci(NW_quantile_pred[x]) for x in quantiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40fa75-a52a-45f9-8f44-b6097ff621e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3 , figsize=(35,8))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "axes[0].set_title('HF quantiles', fontweight=\"bold\", size=20)\n",
    "axes[1].set_title('LF quantiles', fontweight=\"bold\", size=20)\n",
    "axes[2].set_title('NW quantiles', fontweight=\"bold\", size=20)\n",
    "\n",
    "axes[0].scatter(quantiles, HF_quantile_ex, color='black', s=150)\n",
    "axes[1].scatter(quantiles, LF_quantile_ex, color='black', s=150)\n",
    "axes[2].scatter(quantiles, NW_quantile_ex, color='black', s=150)\n",
    "\n",
    "axes[0].fill_between(quantiles,\n",
    "                HF_predicted_bci[:, 0],\n",
    "                HF_predicted_bci[:, 1],\n",
    "                HF_predicted_bci[:, 0] < HF_predicted_bci[:, 1],  color = 'gold', alpha=0.3)\n",
    "\n",
    "axes[1].fill_between(quantiles,\n",
    "                LF_predicted_bci[:, 0],\n",
    "                LF_predicted_bci[:, 1],\n",
    "                LF_predicted_bci[:, 0] < LF_predicted_bci[:, 1],  color = 'lightskyblue', alpha=0.3)\n",
    "\n",
    "axes[2].fill_between(quantiles,\n",
    "                NW_predicted_bci[:, 0],\n",
    "                NW_predicted_bci[:, 1],\n",
    "                NW_predicted_bci[:, 0] < NW_predicted_bci[:, 1],  color = 'limegreen', alpha=0.3)\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "        ax.set_xlabel('Quantiles', fontsize=20)\n",
    "        ax.set_xticks(quantiles)\n",
    "        ax.set_xticklabels(quantiles)\n",
    "        ax.set_ylabel('RTs upper boundary', fontsize=20)\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "                tick.label.set_fontsize(16)\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16) \n",
    "\n",
    "sns.despine()\n",
    "plt.savefig(plots_path + 'Quantiles Poseterior.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d335303-115f-4be3-b5b9-44db89033c76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mean Accuracy and RT Posterior Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60fb04-d913-45b7-aa6b-86aac6e69dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_data_rt_mean = HF_data['rt'].mean()\n",
    "LF_data_rt_mean = LF_data['rt'].mean()\n",
    "NW_data_rt_mean = NW_data['rt'].mean()\n",
    "\n",
    "HF_pred_rt_mean = HF_pred_rt.mean(axis=0)\n",
    "LF_pred_rt_mean = LF_pred_rt.mean(axis=0)\n",
    "NW_pred_rt_mean = NW_pred_rt.mean(axis=0)\n",
    "\n",
    "\n",
    "HF_data_resp_mean = HF_data['response'].mean()\n",
    "LF_data_resp_mean = LF_data['response'].mean()\n",
    "NW_data_resp_mean = NW_data['response'].mean()\n",
    "\n",
    "HF_pred_resp_mean = HF_pred_resp.mean(axis=0)\n",
    "LF_pred_resp_mean = LF_pred_resp.mean(axis=0)\n",
    "NW_pred_resp_mean = NW_pred_resp.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28be60d-fa47-4f5b-bb8e-3e9ea312239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(x, data_mean, ax):\n",
    "    \"\"\"\n",
    "    Plots the posterior of x with experimental data mean as a line\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        An array containing RT or response for each trial.\n",
    "        \n",
    "    x : float\n",
    "        mean of RT or Accuracy of experimental data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    density = gaussian_kde(x, bw_method='scott')\n",
    "    xd = np.linspace(x.min(), x.max())\n",
    "    yd = density(xd)\n",
    "\n",
    "    low, high = bci(x)\n",
    "    ax.fill_between(xd[np.logical_and(xd >= low, xd <= high)],\n",
    "                     yd[np.logical_and(xd >= low, xd <= high)], color = 'lightsteelblue')\n",
    "\n",
    "    ax.plot(xd, yd, color='slategray')\n",
    "    ax.axvline(data_mean, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ee9b7-ea0d-4ef3-8ba8-8591e9bb437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,2 , figsize=(15,20))\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "\n",
    "axes[0][0].set_title('HF mean RT', fontweight=\"bold\", size=16)\n",
    "axes[0][1].set_title('HF mean Response', fontweight=\"bold\", size=16)\n",
    "axes[1][0].set_title('LF mean RT', fontweight=\"bold\", size=16)\n",
    "axes[1][1].set_title('LF mean Response', fontweight=\"bold\", size=16)\n",
    "axes[2][0].set_title('NW mean RT', fontweight=\"bold\", size=16)\n",
    "axes[2][1].set_title('NW mean Response', fontweight=\"bold\", size=16)\n",
    "\n",
    "plot_posterior(HF_pred_rt_mean, HF_data_rt_mean, axes[0][0])\n",
    "plot_posterior(HF_pred_resp_mean, HF_data_resp_mean, axes[0][1])\n",
    "\n",
    "plot_posterior(LF_pred_rt_mean, LF_data_rt_mean, axes[1][0])\n",
    "plot_posterior(LF_pred_resp_mean, LF_data_resp_mean, axes[1][1])\n",
    "\n",
    "plot_posterior(NW_pred_rt_mean, NW_data_rt_mean, axes[2][0])\n",
    "plot_posterior(NW_pred_resp_mean, NW_data_resp_mean, axes[2][1])\n",
    "\n",
    "for ax in axes:\n",
    "        ax[0].set_xlabel('RT', fontsize=15)\n",
    "        ax[1].set_xlabel('Accuracy', fontsize=15)\n",
    "        ax[0].set_ylabel('Density', fontsize=15)\n",
    "        ax[1].set_ylabel('Density', fontsize=15)\n",
    "        for tick in ax[0].xaxis.get_major_ticks():\n",
    "                tick.label.set_fontsize(13)\n",
    "        for tick in ax[0].yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(13)\n",
    "        for tick in ax[1].xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(13)\n",
    "        for tick in ax[1].yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(13) \n",
    "\n",
    "plt.savefig(plots_path + 'Mean Accuracy and RT.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae939093-641c-4f79-a8ef-47d8fc9eecba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Estimation_Hier.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "cmdstan",
   "language": "python",
   "name": "cmdstan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
