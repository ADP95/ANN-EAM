{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5a720b-af17-4312-b543-71f603ddfa23",
   "metadata": {
    "id": "04ab4fbd-f191-427a-a2e4-db4ff76fa385"
   },
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40530b-0436-4965-908d-e709114e316b",
   "metadata": {
    "executionInfo": {
     "elapsed": 3102,
     "status": "ok",
     "timestamp": 1651313252929,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "42e504d1-66bf-459a-bdee-ef0f268a7b81"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cmdstanpy \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import arviz as az\n",
    "import os\n",
    "import json\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fda25d-8c73-4076-a32e-b9f64a89752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"Times New Roman\"] + plt.rcParams[\"font.serif\"]\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['pdf.use14corefonts'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5ed11-783c-476d-b190-95c7356783ca",
   "metadata": {},
   "source": [
    "## Choose Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5ad7e-f13d-41a9-a8d5-528f99513a33",
   "metadata": {},
   "source": [
    "#### roots and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c30c41-a570-411a-a874-36f0776f87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './'\n",
    "plots_root = root + 'Plots/'\n",
    "datasets_root = root + 'Datasets/'\n",
    "behavioural_data_root = root +  'behavioral_data/selected_data/' \n",
    "stan_files_root = root +  'stan files/' \n",
    "saved_models_root = root + 'stan_results/'\n",
    "\n",
    "model_config = {}\n",
    "plots_path = ''\n",
    "dataset_path = ''\n",
    "stan_file_path = ''\n",
    "stan_output_dir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fddb96-7bae-4d47-a322-e42e6285e69e",
   "metadata": {},
   "source": [
    "#### read models configuration json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c3c67-1630-4266-9163-296e244195b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rdm_models.json\") as f:\n",
    "    models = json.load(f)\n",
    "    models_name = list(models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768a530-f38b-4267-9084-5ba80a1471af",
   "metadata": {},
   "source": [
    "#### Choose and set model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122cea8-7777-4a9c-98be-4fea9b2dfcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SetModelAndPaths(model_name):\n",
    "    global model_config\n",
    "    global plots_path\n",
    "    global dataset_path\n",
    "    global stan_file_path\n",
    "    global stan_output_dir\n",
    "    model_config = models[model_name]\n",
    "    plots_path = plots_root + model_config['plots_folder_name'] + '/'\n",
    "    dataset_path = datasets_root + model_config['dataset_name']\n",
    "    stan_file_path = stan_files_root + model_config['stan_file']\n",
    "    stan_output_dir = saved_models_root + model_config['model_name'] + '/'\n",
    "    os.path\n",
    "    \n",
    "    if not os.path.exists(plots_path):\n",
    "        os.makedirs(plots_path)\n",
    "        print(\"Directory \" , plots_path ,  \" Created \")\n",
    "    else:    \n",
    "        print(\"Directory \" , plots_path ,  \" already exists\")\n",
    "        \n",
    "    if not os.path.exists(stan_output_dir):\n",
    "        os.makedirs(stan_output_dir)\n",
    "        print(\"Directory \" , stan_output_dir ,  \" Created \")\n",
    "    else:    \n",
    "        print(\"Directory \" , stan_output_dir ,  \" already exists\")\n",
    "\n",
    "widgets.interact(SetModelAndPaths, model_name=models_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3e1780-916e-4904-a2d3-389b27227c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e617f6",
   "metadata": {
    "id": "123c4809-7b46-4f8d-b578-0d5e9fb5fbe7"
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d41247-a08b-4710-a8cb-006ca5ace314",
   "metadata": {},
   "source": [
    "Loading words and non-words with zipf and predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6417ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1651313252934,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "72172233-0e82-4058-8a5c-8657e9fe4693",
    "outputId": "35336463-3bb2-41e5-c84f-7c34fdcfc77d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_nword_df = pd.read_csv(dataset_path, header=None, names =['string', 'freq',  'label', 'zipf','category', 'word_prob', 'non_word_prob'])\n",
    "word_nword_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a061e32-1d3d-40c2-afcf-04de82e44ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, max_rt, min_rt, std_c=2.5):\n",
    "    \"\"\"\n",
    "    Returns remove outliers from dataframes. Outlier RTs are bigger than\n",
    "    max_rt and smaller than min_rt. Also RTsthat are out of -/+ (std_c * sd) \n",
    "    of mean RT interval are considered as outliers too.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        df: pandas dataframe with rt column\n",
    "        max_rt (float): maximum acceptable rt\n",
    "        min_rt (float): minimum acceptable rt\n",
    "        \n",
    "    Optional Parameters\n",
    "    ----------\n",
    "        std_c (float) : Optional\n",
    "            coefficient to define interval of non-outlier RTs\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        df: pandas dataframe without outliers  \n",
    "    \"\"\"\n",
    "    mean = df['rt'].mean()\n",
    "    sd = df['rt'].std()\n",
    "    lower_thr = mean - std_c*sd\n",
    "    upper_thr = mean + std_c*sd\n",
    "    min_bound = max(min_rt, lower_thr)\n",
    "    max_bound = min(max_rt, upper_thr)\n",
    "    df = df[df['rt'] >= min_bound]\n",
    "    df = df[df['rt'] <= max_bound]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf7c47-7fd4-4bdb-880b-5a52a2b7be4b",
   "metadata": {},
   "source": [
    "Reading and modifing each behavioral data file and combining all of them into a single behavioral dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a914e",
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1651313252936,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "eb864830-ecb7-48f1-a071-f00173154216"
   },
   "outputs": [],
   "source": [
    "Number_Of_Participants = 5\n",
    "Number_Of_Trials = 400\n",
    "dataframes = []\n",
    "\n",
    "for i in range(Number_Of_Participants):\n",
    "    # Loading each file\n",
    "    df = pd.read_csv(behavioural_data_root + str(i+1) + \"DATA.LDT\", names=['trial', 'string_id', 'string_type', 'accuracy', 'rt', 'string'])\n",
    "    # Dropping non rows and first two rows that are demographic informations \n",
    "    df = df.dropna().drop('string_id', axis=1).drop([0, 1]).iloc[:Number_Of_Trials] \n",
    "    # Converting columns type to suitable data types\n",
    "    convert_dict = {'string_type': 'int16',\n",
    "                    'accuracy': 'int16',\n",
    "                    'rt': float\n",
    "                   }\n",
    "\n",
    "    df = df.astype(convert_dict)\n",
    "    # Convert RTs to seconds\n",
    "    df['rt'] = df['rt'].apply(lambda x: x/1000) \n",
    "    # Removing Outliers\n",
    "    df = remove_outliers(df, 3, .2, 2.5)\n",
    "    # Extracting response of participant from his/her accuracy\n",
    "    df['response'] = np.logical_not(np.logical_xor(df['string_type'], df['accuracy'])).astype('int')\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Particpant number\n",
    "    df['participant'] = i+1\n",
    "    # Minimum RT of participant in all trials (is needed for stan code)\n",
    "    df['minRT'] = df['rt'].min()\n",
    "    dataframes.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6266a",
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1651313252938,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "f37d4118-f2ea-4691-bff9-02f1ac1cebbc"
   },
   "outputs": [],
   "source": [
    "# Combining dataframes\n",
    "behavioural_df = pd.concat(dataframes)\n",
    "# Merging  behavioral dataframe with word_nonword_df to have words and non-words data with behavioral data\n",
    "behavioural_df = pd.merge(behavioural_df, word_nword_df, on='string', how='left').dropna().reset_index(drop=True)\n",
    "behavioural_df = behavioural_df.drop([\"trial\", \"string_type\", \"freq\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c791ac-26e5-4ee4-929e-379e1b3248df",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioural_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159dddd5-c8a8-4c1a-8c48-9882ee452eb2",
   "metadata": {},
   "source": [
    "Predicted probabilities of words and non-words in different conditions in all trials\n",
    "across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1441c1-4928-4675-92a0-bac19c71fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioural_df.groupby(['category']).agg({'word_prob': ['mean', 'std', 'count', 'max', 'min'], 'non_word_prob': ['mean', 'std', 'count', 'max', 'min']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052f5c8-5b10-4c7d-ad5c-27e781c292fb",
   "metadata": {},
   "source": [
    "RT and response description of words and non-words in different conditions in all trials\n",
    "across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3741e-8006-4460-a23d-67445b949a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioural_df.groupby(['category']).agg({'rt': ['mean', 'std', 'max', 'min'], 'response': ['mean', 'std', 'max', 'min']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6ff4c",
   "metadata": {
    "id": "ff61f25e-7817-4f3f-a48f-d5a6666b0e75",
    "tags": []
   },
   "source": [
    "## Stan Model and Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa0a87-ed95-486a-beec-87a50c2f86ef",
   "metadata": {},
   "source": [
    "Compiling stan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60550ee7-157d-4393-9f12-773eb947efce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93380,
     "status": "ok",
     "timestamp": 1651313346844,
     "user": {
      "displayName": "Arash Dadras",
      "userId": "05722346836148265544"
     },
     "user_tz": -270
    },
    "id": "c1b3627d-0ea2-49b6-86f0-7b098a5a16d6",
    "outputId": "4bc8643c-82c0-4fc9-99a0-7f232fa1eea4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdm_model = cmdstanpy.CmdStanModel(model_name=model_config['model_name'],\n",
    "                                   stan_file=stan_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75c8503-9cb2-40c0-ae79-e0e8d386a86f",
   "metadata": {},
   "source": [
    "Preparing model's inputs\n",
    "\n",
    "note that some inputs of data_dict might not be used depending on which model is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38af62-fefa-4440-b250-f1717e0a2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(behavioural_df)                                                    # For all models\n",
    "participant = behavioural_df['participant'].to_numpy()                     # For all models\n",
    "p = behavioural_df.loc[:, ['word_prob', 'non_word_prob']].to_numpy()       # predicted probabilites of words and non-words, for ANN-EAM models\n",
    "frequency = behavioural_df['zipf'].to_numpy().astype(int)                  # zipf values, for models with non-decision time or drift modulation\n",
    "frequencyCondition = behavioural_df['category'].replace([\"HF\", \"LF\", \"NW\"], [1, 2, 3]).to_numpy() # For models with conditional drift\n",
    "response = behavioural_df['response'].to_numpy().astype(int)               # for all models\n",
    "rt = behavioural_df['rt'].to_numpy()                                       # for all models\n",
    "minRT = behavioural_df['minRT'].to_numpy()                                 # for all models\n",
    "RTbound = 0.1                                                              # for all models\n",
    "\n",
    "threshold_priors = [0, 1, 1, 1]          # For all models with LBA\n",
    "ndt_priors = [0, 1, 1, 1];               # For models wtihout non-decision time modulation\n",
    "g_priors = [-2, 1, 0, 1]                 # For models wtih non-decision time modulation\n",
    "m_priors = [0, 0.5, 0, 1]                # For models wtih non-decision time modulation\n",
    "drift_priors = [1, 2, 1, 1]              # For models without drift mapping functions (non ANN-EAM models)\n",
    "alpha_priors = [0, 1, 1, 1]              # For models with drift mapping functions\n",
    "b_priors = [0, 1, 1, 1]                  # For models with drift mapping functions with asymptote modulation and linear models\n",
    "k_priors = [2, 1, 1, 1]                  # For models with sigmoid drift mapping functions (ANN-EAM models)\n",
    "\n",
    "# define input for the model\n",
    "data_dict = {'N': N,\n",
    "             'L': Number_Of_Participants,\n",
    "             'participant': participant,\n",
    "             'response': response,\n",
    "             'rt': rt,\n",
    "             'minRT': minRT,\n",
    "             'RTbound': RTbound,\n",
    "             'frequency': frequency,\n",
    "             'frequencyCondition': frequencyCondition,\n",
    "             'threshold_priors': threshold_priors,\n",
    "             'ndt_priors': ndt_priors,\n",
    "             'g_priors': g_priors,\n",
    "             'm_priors': m_priors,\n",
    "             'drift_priors': drift_priors,\n",
    "             'p': p,\n",
    "             'alpha_priors': alpha_priors,\n",
    "             'b_priors': b_priors,\n",
    "             'k_priors': k_priors,\n",
    "             }\n",
    "\n",
    "# set sampling parameters\n",
    "n_iter = 500\n",
    "n_warmup = int(n_iter/2)\n",
    "n_sample = int(n_iter/2)\n",
    "n_chains = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4034d3d-99d8-43bf-b744-6b0608141c8d",
   "metadata": {},
   "source": [
    "Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddd68e",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "edac015c-18f4-4ebf-8c3c-ecaf4b8e1d6a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit = rdm_model.sample(data=data_dict,\n",
    "                       iter_sampling=n_sample, \n",
    "                       iter_warmup=n_warmup,\n",
    "                       chains=n_chains,\n",
    "                       output_dir=stan_output_dir,\n",
    "                       show_console=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7bfe3-f4b0-4c5e-b38a-da896fc9910a",
   "metadata": {},
   "source": [
    "## Model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4b9f0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a8e1d56e-e7e4-4ca1-a0bd-5b34edf6ea44"
   },
   "outputs": [],
   "source": [
    "print(\"***hmc diagnostics:\")\n",
    "print(fit.diagnose(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033c45b",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "efa2a6ff-ad48-4c7e-a094-4c645923644e"
   },
   "outputs": [],
   "source": [
    "df = fit.summary()\n",
    "\n",
    "print(\"***DF: \")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302aa3b",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "b33a48e1-8c3b-4668-9e74-ba74c58e3bec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "print(\"***Rhat > 1.01: \")\n",
    "for f in df[\"R_hat\"]:\n",
    "    if f >= 1.01 or f <= 0.9:\n",
    "        counter+=1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54606c2d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "681ed2bf-2a28-4a8b-90d0-24b9fede846f"
   },
   "outputs": [],
   "source": [
    "df.loc[df['R_hat']>1.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa820a",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "a651e5b7-b7f1-4426-ae8e-f9e236eb6e91"
   },
   "outputs": [],
   "source": [
    "df.loc[df['R_hat']>1.01].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32ef5c",
   "metadata": {
    "id": "2743cc7e-7238-4f84-afe4-4b502770f62b",
    "tags": []
   },
   "source": [
    "## Check parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aaf784-e97d-4d81-bcb9-90e7eefff98f",
   "metadata": {},
   "source": [
    "Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a33d647",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "d781cdef-a1c6-4af1-940c-824398ec7d2b"
   },
   "outputs": [],
   "source": [
    "# fit = cmdstanpy.from_csv(stan_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc740f54-db93-410c-9e26-282710ef2024",
   "metadata": {},
   "source": [
    "Parameters posterior plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67254033",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "48fc6630-74be-47a0-b387-ec99b9dc4b47"
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(fit, var_names=model_config['transf_params'], hdi_prob=.95);\n",
    "plt.savefig(plots_path + 'Parameters.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49f9c5",
   "metadata": {},
   "source": [
    "Loading model parameters for each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133df71d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3539c070-29f7-422d-8c2e-1f7d44ef9246"
   },
   "outputs": [],
   "source": [
    "drift_word_t = fit.stan_variables()['drift_word_t']\n",
    "drift_nonword_t = fit.stan_variables()['drift_nonword_t']\n",
    "if model_config['model_name'] != \"RDM\":\n",
    "    threshold_t_word = fit.stan_variables()['threshold_t_word']\n",
    "    threshold_t_nonword = fit.stan_variables()['threshold_t_nonword']\n",
    "else:\n",
    "    threshold_t = fit.stan_variables()['threshold_t']\n",
    "ndt_t = fit.stan_variables()['ndt_t']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaeee5d-28e4-4b67-9d90-6d8ba5d39423",
   "metadata": {},
   "source": [
    "#### Models mean parameters in different conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c645f86a",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3a247d68-c3af-49ef-91fd-ba4ea221358d"
   },
   "outputs": [],
   "source": [
    "HF_condition_w = drift_word_t[:, behavioural_df['category']==\"HF\"]\n",
    "HF_condition_nw = drift_nonword_t[:, behavioural_df['category']==\"HF\"]\n",
    "LF_condition_w = drift_word_t[:, behavioural_df['category']==\"LF\"]\n",
    "LF_condition_nw = drift_nonword_t[:, behavioural_df['category']==\"LF\"]\n",
    "NW_condition_w = drift_word_t[:, behavioural_df['category']==\"NW\"]\n",
    "NW_condition_nw = drift_nonword_t[:, behavioural_df['category']==\"NW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e000d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "deac1183-5610-4a19-8ff8-7bace8d12e7c"
   },
   "outputs": [],
   "source": [
    "print('HF words, word drift mean and std:')\n",
    "print(np.mean(np.mean(HF_condition_w, axis=1)), np.std(np.mean(HF_condition_w, axis=1)))\n",
    "print('HF words, nonword drift mean and std:')\n",
    "print(np.mean(np.mean(HF_condition_nw, axis=1)), np.std(np.mean(HF_condition_nw, axis=1)))\n",
    "print('LF words word drift mean and std:')\n",
    "print(np.mean(np.mean(LF_condition_w, axis=1)), np.std(np.mean(LF_condition_w, axis=1)))\n",
    "print('LF words nonword drift mean and std:')\n",
    "print(np.mean(np.mean(LF_condition_nw, axis=1)), np.std(np.mean(LF_condition_nw, axis=1)))\n",
    "print('NW words word drift mean and std:')\n",
    "print(np.mean(np.mean(NW_condition_w, axis=1)), np.std(np.mean(NW_condition_w, axis=1)))\n",
    "print('NW words nonword drift mean and std:')\n",
    "print(np.mean(np.mean(NW_condition_nw, axis=1)), np.std(np.mean(NW_condition_nw, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84308d46-e368-491f-8532-0f2b7b4f9966",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3a247d68-c3af-49ef-91fd-ba4ea221358d"
   },
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"RDM\":\n",
    "    HF_condition_w = threshold_t_word[:, behavioural_df['category']==\"HF\"]\n",
    "    HF_condition_nw = threshold_t_nonword[:, behavioural_df['category']==\"HF\"]\n",
    "    LF_condition_w = threshold_t_word[:, behavioural_df['category']==\"LF\"]\n",
    "    LF_condition_nw = threshold_t_nonword[:, behavioural_df['category']==\"LF\"]\n",
    "    NW_condition_w = threshold_t_word[:, behavioural_df['category']==\"NW\"]\n",
    "    NW_condition_nw = threshold_t_nonword[:, behavioural_df['category']==\"NW\"]\n",
    "else:\n",
    "    HF_condition = threshold_t[:, behavioural_df['category']==\"HF\"]\n",
    "    LF_condition = threshold_t[:, behavioural_df['category']==\"LF\"]\n",
    "    NW_condition = threshold_t[:, behavioural_df['category']==\"NW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d462da-a2d7-403e-9e79-1bcfd84251db",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "deac1183-5610-4a19-8ff8-7bace8d12e7c"
   },
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"RDM\":\n",
    "    print('HF words, word threshold mean and std:')\n",
    "    print(np.mean(np.mean(HF_condition_w, axis=1)), np.std(np.mean(HF_condition_w, axis=1)))\n",
    "    print('HF words, nonword threshold mean and std:')\n",
    "    print(np.mean(np.mean(HF_condition_nw, axis=1)), np.std(np.mean(HF_condition_nw, axis=1)))\n",
    "    print('LF words word threshold mean and std:')\n",
    "    print(np.mean(np.mean(LF_condition_w, axis=1)), np.std(np.mean(LF_condition_w, axis=1)))\n",
    "    print('LF words nonword threshold mean and std:')\n",
    "    print(np.mean(np.mean(LF_condition_nw, axis=1)), np.std(np.mean(LF_condition_nw, axis=1)))\n",
    "    print('NW words word threshold mean and std:')\n",
    "    print(np.mean(np.mean(NW_condition_w, axis=1)), np.std(np.mean(NW_condition_w, axis=1)))\n",
    "    print('NW words nonword threshold mean and std:')\n",
    "    print(np.mean(np.mean(NW_condition_nw, axis=1)), np.std(np.mean(NW_condition_nw, axis=1)))\n",
    "else:\n",
    "    print('HF words, threshold mean and std:')\n",
    "    print(np.mean(np.mean(HF_condition, axis=1)), np.std(np.mean(HF_condition, axis=1)))\n",
    "    print('LF words, threshold mean and std:')\n",
    "    print(np.mean(np.mean(LF_condition, axis=1)), np.std(np.mean(LF_condition, axis=1)))\n",
    "    print('NW words, word threshold mean and std:')\n",
    "    print(np.mean(np.mean(NW_condition, axis=1)), np.std(np.mean(NW_condition, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914e74a9",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "06aceb06-2f53-47ae-b0f8-68d0c96f3cbf"
   },
   "outputs": [],
   "source": [
    "HF_condition = ndt_t[:, behavioural_df['category']==\"HF\"]\n",
    "LF_condition = ndt_t[:, behavioural_df['category']==\"LF\"]\n",
    "NW_condition = ndt_t[:, behavioural_df['category']==\"NW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacfbe0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "c7ff4f51-0587-4f7a-bf79-7bc1fe0951e2"
   },
   "outputs": [],
   "source": [
    "print('HF words ndt_t mean and std:')\n",
    "print(np.mean(np.mean(HF_condition, axis=1)), np.std(np.mean(HF_condition, axis=1)))\n",
    "print('LF words ndt_t mean and std:')\n",
    "print(np.mean(np.mean(LF_condition, axis=1)), np.std(np.mean(LF_condition, axis=1)))\n",
    "print('Non Words ndt_t mean and std:')\n",
    "print(np.mean(np.mean(NW_condition, axis=1)), np.std(np.mean(NW_condition, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64f181-de50-4bec-ad6a-9f0c7c95cea5",
   "metadata": {},
   "source": [
    "## Calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018e8df-553d-42b4-833c-d2f967b17ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_waic(log_likelihood, pointwise=False):\n",
    "    \"\"\"\n",
    "    Returns model comparisions' metrics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        log_likelihood: np.array\n",
    "            log_likelihood of each trial\n",
    "        max_rt: float\n",
    "            maximum acceptable rt\n",
    "        min_rt: float\n",
    "             minimum acceptable rt\n",
    "             \n",
    "    Optional Parameters\n",
    "    ----------------\n",
    "    pointwise: float\n",
    "        if true pointwise waic will be calculated\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        out:  a dictionary containing lppd, waic, waic_se and pointwise_waic    \n",
    "    \"\"\"\n",
    "    likelihood = np.exp(log_likelihood)\n",
    "\n",
    "    mean_l = np.mean(likelihood, axis=0) # N observations\n",
    "\n",
    "    pointwise_lppd = np.log(mean_l)\n",
    "    lppd = np.sum(pointwise_lppd)\n",
    "\n",
    "    pointwise_var_l = np.var(log_likelihood, axis=0) # N observations\n",
    "    var_l = np.sum(pointwise_var_l)\n",
    "\n",
    "    pointwise_waic = - 2*pointwise_lppd +  2*pointwise_var_l\n",
    "    waic = -2*lppd + 2*var_l\n",
    "    waic_se = np.sqrt(log_likelihood.shape[1] * np.var(pointwise_waic))\n",
    "\n",
    "    if pointwise:\n",
    "        out = {'lppd':lppd,\n",
    "               'p_waic':var_l,\n",
    "               'waic':waic,\n",
    "               'waic_se':waic_se,\n",
    "               'pointwise_waic':pointwise_waic}\n",
    "    else:\n",
    "        out = {'lppd':lppd,\n",
    "               'p_waic':var_l,\n",
    "                'waic':waic,\n",
    "                'waic_se':waic_se}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beabf75-4cb8-49dc-b604-9f0adf32dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood = fit.stan_variables()['log_lik']\n",
    "print(calculate_waic(log_likelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a23772b-94de-40e6-88d6-f5a6aaefab64",
   "metadata": {},
   "source": [
    "## Simulating RDM with estimated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737be2f5-fd9f-432a-a42f-c8bc89a28c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rdm_2A(cor_drift, inc_drift, threshold_word, threshold_nonword, ndt, noise_constant=1, dt=0.001, max_rt=10):\n",
    "    \"\"\" \n",
    "    Simulates behavior (rt and accuracy) according to the Racing Diffusion Model.\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    cor_drift : numpy.ndarray\n",
    "        Drift-rate of the Racing Diffusion Model - correct trials.\n",
    "    inc_drift : numpy.ndarray\n",
    "        Drift-rate of the Racing Diffusion Model - incorrect trials.\n",
    "    threshold : numpy.ndarray\n",
    "        Shape is usually (n_samples, n_trials).\n",
    "        Threshold of the diffusion decision model.\n",
    "    ndt : numpy.ndarray\n",
    "        Shape is usually (n_samples, n_trials).\n",
    "        Non decision time of the diffusion decision model, in seconds.\n",
    "    \n",
    "    Optional Parameters\n",
    "    ----------------\n",
    "    noise_constant : float, default 1\n",
    "        Scaling factor of the Racing Diffusion Model.\n",
    "        If changed, drift and threshold would be scaled accordingly.\n",
    "        Not to be changed in most applications.\n",
    "    dt : float, default 0.001\n",
    "        Controls the time resolution of the Racing Diffusion Model. Default is 1 msec.\n",
    "        Lower values of dt make the function more precise but much slower.\n",
    "    max_rt : float, default 10\n",
    "        Controls the maximum rts that can be predicted.\n",
    "        Making this higher might make the function a bit slower.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rt : numpy.ndarray\n",
    "        Shape is the same as the input parameters.\n",
    "        Contains simulated response times according to the Racing Diffusion Model.\n",
    "        Every element corresponds to the set of parameters given as input with the same shape.\n",
    "    acc: numpy.ndarray\n",
    "        Shape is the same as the input parameters.\n",
    "        Contains simulated accuracy according to the Racing Diffusion Model.\n",
    "        Every element corresponds to the set of parameters given as input with the same shape.\n",
    "    \"\"\"\n",
    "    shape = cor_drift.shape\n",
    "    acc = np.empty(shape)\n",
    "    rt = np.empty(shape)\n",
    "    acc[:] = np.nan\n",
    "    rt[:] = np.nan\n",
    "\n",
    "    max_tsteps = max_rt/dt\n",
    "\n",
    "    x_cor = np.zeros(shape)\n",
    "    x_inc = np.zeros(shape)\n",
    "\n",
    "    tstep = 0\n",
    "    ongoing = np.array(np.ones(shape), dtype=bool)\n",
    "\n",
    "    stop_race = False\n",
    "\n",
    "    while np.sum(ongoing) > 0 and tstep < max_tsteps:\n",
    "        x_cor[ongoing] += np.random.normal(cor_drift[ongoing]*dt,\n",
    "                                           noise_constant*np.sqrt(dt),\n",
    "                                           np.sum(ongoing))\n",
    "        x_inc[ongoing] += np.random.normal(inc_drift[ongoing]*dt,\n",
    "                                           noise_constant*np.sqrt(dt),\n",
    "                                           np.sum(ongoing))\n",
    "        tstep += 1\n",
    "        ended_correct = (x_cor >= threshold_word)\n",
    "        ended_incorrect = (x_inc >= threshold_nonword)\n",
    "\n",
    "        # store results and filter out ended trials\n",
    "        if np.sum(ended_correct) > 0:\n",
    "            acc[np.logical_and(ended_correct, ongoing)] = 1\n",
    "            rt[np.logical_and(ended_correct, ongoing)] = dt*tstep + ndt[np.logical_and(ended_correct, ongoing)]\n",
    "            ongoing[ended_correct] = False\n",
    "\n",
    "        if np.sum(ended_incorrect) > 0:\n",
    "            acc[np.logical_and(ended_incorrect, ongoing)] = 0\n",
    "            rt[np.logical_and(ended_incorrect, ongoing)] = dt*tstep + ndt[np.logical_and(ended_incorrect, ongoing)]\n",
    "            ongoing[ended_incorrect] = False\n",
    "    return rt, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867e05c-ebda-4bc3-ae51-e340e337344f",
   "metadata": {},
   "source": [
    "Simulating RDM with estimated parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6786ed3-265f-4698-9a37-75d19e125806",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_config['model_name'] != \"RDM\":\n",
    "    pp_rt, pp_response = random_rdm_2A(drift_word_t, drift_nonword_t, threshold_t_word, threshold_t_nonword, ndt_t, noise_constant=1, dt=0.001, max_rt=5)\n",
    "else:\n",
    "    pp_rt, pp_response = random_rdm_2A(drift_word_t, drift_nonword_t, threshold_t, threshold_t, ndt_t, noise_constant=1, dt=0.001, max_rt=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7183cf1-fc0e-436b-b607-9d821ef736f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bci(x, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate Bayesian credible interval (BCI).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        An array containing MCMC samples.\n",
    "    \n",
    "    Optional Parameters\n",
    "    -------------------\n",
    "    alpha : float, default 0.05\n",
    "        Desired probability of type I error.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    interval : numpy.ndarray\n",
    "        Array containing the lower and upper bounds of the bci interval.\n",
    "    \"\"\"\n",
    "    interval = np.nanpercentile(x, [(alpha/2)*100, (1-alpha/2)*100])\n",
    "\n",
    "    return interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235c705-7301-4c3c-b906-5979789d606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted Data\n",
    "rt_predictions = pd.concat((pd.DataFrame(pp_rt, index=pd.Index(np.arange(1, len(pp_rt)+1))).T, behavioural_df['category']), axis=1)\n",
    "response_predictions = pd.concat((pd.DataFrame(pp_response, index=pd.Index(np.arange(1, len(pp_response)+1))).T, behavioural_df['category']), axis=1)\n",
    "\n",
    "# Experiment Data\n",
    "experiment_data = behavioural_df.loc[:, ['rt', 'response', 'category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd192d-bac0-4fdc-afd2-148b6a9ae332",
   "metadata": {},
   "source": [
    "Separating RT and Response of predicted and experimental data for each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd58c1-f474-4c7c-85d6-f26b16c77859",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_data = experiment_data.loc[experiment_data['category']=='HF']\n",
    "LF_data = experiment_data.loc[experiment_data['category']=='LF']\n",
    "NW_data = experiment_data.loc[experiment_data['category']=='NW']\n",
    "\n",
    "HF_pred_rt = rt_predictions.loc[rt_predictions['category']=='HF'].drop(['category'], axis=1)\n",
    "HF_pred_resp = response_predictions.loc[response_predictions['category']=='HF'].drop(['category'], axis=1)\n",
    "LF_pred_rt = rt_predictions.loc[rt_predictions['category']=='LF'].drop(['category'], axis=1)\n",
    "LF_pred_resp = response_predictions.loc[response_predictions['category']=='LF'].drop(['category'], axis=1)\n",
    "NW_pred_rt = rt_predictions.loc[rt_predictions['category']=='NW'].drop(['category'], axis=1)\n",
    "NW_pred_resp = response_predictions.loc[response_predictions['category']=='NW'].drop(['category'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68071a-3121-47da-94c5-bae5db334c5c",
   "metadata": {
    "id": "defea622-f638-4a0e-a269-937234d4a49f",
    "tags": []
   },
   "source": [
    "## Quantiles Posterior Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378eb36-76cc-4a39-b2ed-643addccbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [.1, .3, .5, .7, .9]\n",
    "percentiles = np.array(quantiles)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96001a51-55af-4f25-93c4-cdcc00556eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment Data quantile\n",
    "HF_quantile_ex = HF_data['rt'].quantile(quantiles)\n",
    "LF_quantile_ex = LF_data['rt'].quantile(quantiles)\n",
    "NW_quantile_ex = NW_data['rt'].quantile(quantiles)\n",
    "\n",
    "# predicted data quantiles (for each sample)\n",
    "HF_quantile_pred = HF_pred_rt.quantile(quantiles, axis=0).T\n",
    "LF_quantile_pred = LF_pred_rt.quantile(quantiles, axis=0).T\n",
    "NW_quantile_pred = NW_pred_rt.quantile(quantiles, axis=0).T\n",
    "\n",
    "# predicted data quantiles bci\n",
    "HF_predicted_bci = np.array([bci(HF_quantile_pred[x]) for x in quantiles])\n",
    "LF_predicted_bci = np.array([bci(LF_quantile_pred[x]) for x in quantiles])\n",
    "NW_predicted_bci = np.array([bci(NW_quantile_pred[x]) for x in quantiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e40fa75-a52a-45f9-8f44-b6097ff621e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3 , figsize=(35,8))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "\n",
    "axes[0].set_title('HF quantiles', fontweight=\"bold\", size=20)\n",
    "axes[1].set_title('LF quantiles', fontweight=\"bold\", size=20)\n",
    "axes[2].set_title('NW quantiles', fontweight=\"bold\", size=20)\n",
    "\n",
    "axes[0].scatter(quantiles, HF_quantile_ex, color='black', s=150)\n",
    "axes[1].scatter(quantiles, LF_quantile_ex, color='black', s=150)\n",
    "axes[2].scatter(quantiles, NW_quantile_ex, color='black', s=150)\n",
    "\n",
    "axes[0].fill_between(quantiles,\n",
    "                HF_predicted_bci[:, 0],\n",
    "                HF_predicted_bci[:, 1],\n",
    "                HF_predicted_bci[:, 0] < HF_predicted_bci[:, 1],  color = 'gold', alpha=0.3)\n",
    "\n",
    "axes[1].fill_between(quantiles,\n",
    "                LF_predicted_bci[:, 0],\n",
    "                LF_predicted_bci[:, 1],\n",
    "                LF_predicted_bci[:, 0] < LF_predicted_bci[:, 1],  color = 'lightskyblue', alpha=0.3)\n",
    "\n",
    "axes[2].fill_between(quantiles,\n",
    "                NW_predicted_bci[:, 0],\n",
    "                NW_predicted_bci[:, 1],\n",
    "                NW_predicted_bci[:, 0] < NW_predicted_bci[:, 1],  color = 'limegreen', alpha=0.3)\n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "        ax.set_xlabel('Quantiles', fontsize=20)\n",
    "        ax.set_xticks(quantiles)\n",
    "        ax.set_xticklabels(quantiles)\n",
    "        ax.set_ylabel('RTs upper boundary', fontsize=20)\n",
    "        for tick in ax.xaxis.get_major_ticks():\n",
    "                tick.label.set_fontsize(16)\n",
    "        for tick in ax.yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(16) \n",
    "\n",
    "sns.despine()\n",
    "plt.savefig(plots_path + 'Quantiles Poseterior.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d335303-115f-4be3-b5b9-44db89033c76",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mean Accuracy and RT Posterior Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60fb04-d913-45b7-aa6b-86aac6e69dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_data_rt_mean = HF_data['rt'].mean()\n",
    "LF_data_rt_mean = LF_data['rt'].mean()\n",
    "NW_data_rt_mean = NW_data['rt'].mean()\n",
    "\n",
    "HF_pred_rt_mean = HF_pred_rt.mean(axis=0)\n",
    "LF_pred_rt_mean = LF_pred_rt.mean(axis=0)\n",
    "NW_pred_rt_mean = NW_pred_rt.mean(axis=0)\n",
    "\n",
    "\n",
    "HF_data_resp_mean = HF_data['response'].mean()\n",
    "LF_data_resp_mean = LF_data['response'].mean()\n",
    "NW_data_resp_mean = NW_data['response'].mean()\n",
    "\n",
    "HF_pred_resp_mean = HF_pred_resp.mean(axis=0)\n",
    "LF_pred_resp_mean = LF_pred_resp.mean(axis=0)\n",
    "NW_pred_resp_mean = NW_pred_resp.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28be60d-fa47-4f5b-bb8e-3e9ea312239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(x, data_mean, ax):\n",
    "    \"\"\"\n",
    "    Plots the posterior of x with experimental data mean as a line\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        An array containing RT or response for each trial.\n",
    "        \n",
    "    x : float\n",
    "        mean of RT or Accuracy of experimental data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    density = gaussian_kde(x, bw_method='scott')\n",
    "    xd = np.linspace(x.min(), x.max())\n",
    "    yd = density(xd)\n",
    "\n",
    "    low, high = bci(x)\n",
    "    ax.fill_between(xd[np.logical_and(xd >= low, xd <= high)],\n",
    "                     yd[np.logical_and(xd >= low, xd <= high)], color = 'lightsteelblue')\n",
    "\n",
    "    ax.plot(xd, yd, color='slategray')\n",
    "    ax.axvline(data_mean, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ee9b7-ea0d-4ef3-8ba8-8591e9bb437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3,2 , figsize=(15,20))\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.5)\n",
    "\n",
    "axes[0][0].set_title('HF mean RT', fontweight=\"bold\", size=16)\n",
    "axes[0][1].set_title('HF mean Response', fontweight=\"bold\", size=16)\n",
    "axes[1][0].set_title('LF mean RT', fontweight=\"bold\", size=16)\n",
    "axes[1][1].set_title('LF mean Response', fontweight=\"bold\", size=16)\n",
    "axes[2][0].set_title('NW mean RT', fontweight=\"bold\", size=16)\n",
    "axes[2][1].set_title('NW mean Response', fontweight=\"bold\", size=16)\n",
    "\n",
    "plot_posterior(HF_pred_rt_mean, HF_data_rt_mean, axes[0][0])\n",
    "plot_posterior(HF_pred_resp_mean, HF_data_resp_mean, axes[0][1])\n",
    "\n",
    "plot_posterior(LF_pred_rt_mean, LF_data_rt_mean, axes[1][0])\n",
    "plot_posterior(LF_pred_resp_mean, LF_data_resp_mean, axes[1][1])\n",
    "\n",
    "plot_posterior(NW_pred_rt_mean, NW_data_rt_mean, axes[2][0])\n",
    "plot_posterior(NW_pred_resp_mean, NW_data_resp_mean, axes[2][1])\n",
    "\n",
    "for ax in axes:\n",
    "        ax[0].set_xlabel('RT', fontsize=15)\n",
    "        ax[1].set_xlabel('Accuracy', fontsize=15)\n",
    "        ax[0].set_ylabel('Density', fontsize=15)\n",
    "        ax[1].set_ylabel('Density', fontsize=15)\n",
    "        for tick in ax[0].xaxis.get_major_ticks():\n",
    "                tick.label.set_fontsize(13)\n",
    "        for tick in ax[0].yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(13)\n",
    "        for tick in ax[1].xaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(13)\n",
    "        for tick in ax[1].yaxis.get_major_ticks():\n",
    "            tick.label.set_fontsize(13) \n",
    "\n",
    "plt.savefig(plots_path + 'Mean Accuracy and RT.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4a757-7c50-4d9a-8f22-621cdc91f9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Estimation_Hier.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "cmdstan",
   "language": "python",
   "name": "cmdstan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
